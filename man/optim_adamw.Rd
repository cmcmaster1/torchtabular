% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adamw.R
\name{optim_adamw}
\alias{optim_adamw}
\title{Implements AdamW algorithm.}
\usage{
optim_adamw(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  amsgrad = FALSE
)
}
\arguments{
\item{params}{(iterable): iterable of parameters to optimize or dicts defining
parameter groups}

\item{lr}{(float, optional): learning rate (default: 1e-3)}

\item{betas}{(`Tuple[float, float]`, optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))}

\item{eps}{(float, optional): term added to the denominator to improve
numerical stability (default: 1e-8)}

\item{weight_decay}{(float, optional): weight decay (L2 penalty) (default: 0)}

\item{amsgrad}{(boolean, optional): whether to use the AMSGrad variant of this
algorithm from the paper [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)
(default: FALSE)}
}
\description{
It has been proposed in [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).
}
\examples{
\dontrun{
optimizer <- optim_adamw(model$parameters(), lr=0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()
}

}
