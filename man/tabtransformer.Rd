% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tabtransformer.R
\name{tabtransformer}
\alias{tabtransformer}
\title{Tabtransformer}
\usage{
tabtransformer(
  categories,
  num_continuous,
  dim_out = 1,
  final_layer = NULL,
  attention = "both",
  attention_type = "softmax",
  is_first = FALSE,
  dim = 16,
  depth = 4,
  heads_selfattn = 8,
  heads_intersample = 8,
  dim_heads_selfattn = 8,
  dim_heads_intersample = 8,
  attn_dropout = 0.1,
  ff_dropout = 0.8,
  embedding_dropout = 0.1,
  mlp_dropout = 0.1,
  mlp_hidden_mult = c(4, 2),
  softmax_mod = 1,
  is_softmax_mod = 1,
  device = "cuda"
)
}
\arguments{
\item{categories}{(int vector) a vector containing the dimensions of each categorical predictor (in the correct order)}

\item{num_continuous}{(int) the number of continuous predictors}

\item{dim_out}{(int) dimensions of the output (default is 1, matching the default binary task)}

\item{final_layer}{(nn_module) the final layer of the model (e.g. \code{nn_relu()} to constrain
output to values >= 0 only). Default is NULL, which results a in \code{nn_identity()} layer.}

\item{attention}{(str) string value indicating which type(s) of attention to
use, either "both", "mhsa" or "intersample". Default: "both"}

\item{attention_type}{(str) string value indicating either traditional softmax
attention ("softmax") or signed attention ("signed"), which preserves the sign
of the attention heads (negative or positive), so that attention heads can
be interpreted as either being positively or negatively correlated with the
outcome.}

\item{is_first}{(bool) designates whether intersample attention comes before MHSA}

\item{dim}{(int) embedding dimension for categorical and continuous data}

\item{depth}{(int) number of transformer layers}

\item{heads_selfattn}{(int) number of self-attention heads}

\item{heads_intersample}{(int) number of intersample attention heads}

\item{dim_heads_selfattn}{(int) dimensions of the self-attention heads}

\item{dim_heads_intersample}{(int) dimension of the intersample attention heads}

\item{attn_dropout}{(float) dropout percentage for attention layers. Default: 0.1.}

\item{ff_dropout}{(float) dropout percentage for feed-forward layers between attention layers. . Default: 0.1.}

\item{embedding_dropout}{(float) dropout after the embedding layer. Default: 0.1.}

\item{mlp_dropout}{(float) dropout between MLP layers. Default: 0.1.}

\item{mlp_hidden_mult}{(int vector) a numerical vector indicating the hidden dimensions of the final MLP}

\item{softmax_mod}{(float) multiplier for the MHSA softmax function}

\item{is_softmax_mod}{(floart) multiplier for the intersample attention softmax function}

\item{device}{(str) 'cpu' or 'cuda'}
}
\value{
a tabtransformer model
}
\description{
A torch \code{\link[torch]{nn_module}} using multi-headed self attention (MHSA) for tabular datasets.
Additionally, an intersample attention (between rows) layer will be added by setting \code{intersample = FALSE}.
}
\details{
\href{https://arxiv.org/abs/2012.06678}{Huang et al.} introduce MHSA for tabular datasets,
\href{https://arxiv.org/abs/2106.01342}{Somepalli et al.} introduce the concept of intersample attention.
}
\examples{

tabtransformer(
  categories = c(4, 2, 13),
  num_continuous = 6,
  final_layer = nn_relu(),
  depth = 1,
  dim = 32
  )
}
