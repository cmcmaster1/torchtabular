% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tabtransformer.R
\name{tabtransformer}
\alias{tabtransformer}
\title{Tabtransformer}
\usage{
tabtransformer(
  categories,
  num_continuous,
  dim_out = 1,
  task = "binary",
  intersample = TRUE,
  dim = 16,
  depth = 4,
  heads_selfattn = 8,
  heads_intersample = 8,
  dim_heads_selfattn = 8,
  dim_heads_intersample = 8,
  attn_dropout = 0.1,
  ff_dropout = 0.8,
  mlp_hidden_mult = c(4, 2),
  softmax_mod = 1,
  device = "cuda"
)
}
\arguments{
\item{categories}{a vector containing the dimensions of each categorical predictor (in the correct order)}

\item{num_continuous}{the number of continuous predictors}

\item{dim_out}{dimensions of the output (default is 1, matching the default binary task)}

\item{task}{'regression', 'binary' or 'multiclass'}

\item{intersample}{boolean value designating whether to use intersample attention}

\item{dim}{embedding dimension for categorical and continuous data}

\item{depth}{number of transformer layers}

\item{heads_selfattn}{number of self-attention heads}

\item{heads_intersample}{number of intersample attention heads}

\item{dim_heads_selfattn}{dimensions of the self-attention heads}

\item{dim_heads_intersample}{dimension of the intersample attention heads}

\item{attn_dropout}{dropout percentage for attention layers}

\item{ff_dropout}{dropout percentage for feed-forward layers}

\item{mlp_hidden_mult}{a numerical vector indicating the hidden dimensions of the final MLP}

\item{softmax_mod}{multiplier for the attention softmax function}

\item{device}{'cpu' or 'cuda'}
}
\value{
a tabtransformer model
}
\description{
Tabtransformer
}
